---

title: "Machine Learning Part4"
date: 2021-07-05
tags: [machine learning]
site_url: http://inomjonramatov.uz/
header:
   image:"/images/ml-p1/ml-p1-background.jpeg"
categories:
  - Tutorial
excerpt: "ML darsimizning 4-qismiga keldik. Oldingi darslarimizda chiziqli regressiyani davom ettirishimiz va kod yozishimiz haqida gapirgan edim."


---


# Machine Learning part 4:«Underfitting and overfitting» muammolari.
<p style="text-align: justify">ML 4-qismiga keldik. Oldingi darslarimizda chiziqli regressiyani davom ettirishimiz va kod yozishimiz haqida gapirgan edim. Buning uchun sizga python tilini oz muncha tushunishingiz va scikit-learn python based libraryni ham o’rnatishingiz kerak bo’ladi. Buni bajarishni eng sodda yo’li( bu albatta mening shaxsiy fikrim) Anaconda web based envoriment ni o’rnatishingiz deb o’ylayman. Yana bugungi darsimizda MLdagi eng katta muammolardan biri underfitting va overfitting haqida ham gaplashamiz. 
Demak boshladik , eng muhimi sizda python va scikit-learn, numpy, pyplot o’rnatilgan bo’lishi kerak.
<br>Bizga kerakli library larni yuklab olamiz:
</p>
```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
```
<p style="text-align: justify">Biz faqat chiziqli regressiya modulidan foydalib qolmay, numpy (vector va matritsalarni ifodalash uchun ), pyplot (grafiklar chizsh uchun) , kabi (python kutubxonalari,)modullardan foydalanamiz.
Esingizda bo’lsa o’tgan darsda quyidagi malumotlar bazasidan foydalangan edik:
</p>
![alt text](http://inomjonramatov.uz/images/ml3_5.jpg "Machine Learning part4")

<p style="text-align:justify">
Shundan foydalanib o’rganuvchi malumotlar bazamizni tayyorlab olamiz:
</p>
```python
X = np.arange(1, 11).reshape(10, 1)
y = np.array([7, 8, 7, 13, 16, 15, 19, 23, 18, 21]).reshape(10, 1)
```
<p style="text-align:justify">Deyarli hamma Mashinali o’rgnish kutubhonalari yuqoridagi formatda bo’lishi kerak yani har bir qator bitta o’rnaniluvchi misol, va har bir ustun bitta atribut yan belgi.  Keling malumotlarimizni grafik ko'rinishda tasvirlaymiz:</p>
![alt text](http://inomjonramatov.uz/images/ml3_6.jpg "Machine Learning part4")
Endi, chiziqli regressiya modulimizni ishga tushuramiz:
```python
model = LinearRegression()
```
fit funksiyasi orqali o’rganish jarayonini yuqoridagi malumotlar bazasida boshlab yuboramiz:
```python
model.fit(X, y)
```
Bizning malumotlar bazamiz juda kichik va oddiy shunday ekan o’rganish jarayoni juda tez yakunlanadi, va quyida natijalarni ko’rish mumkin:
```python
model.coef_
array([[ 1.77575758]])
model.intercept_
array([ 4.93333333])
```
coef_- og’irliklar uchun(weights) va intercept_ basilar uchun.
<p style="text-align:justify">
Endi basharoat vektori a ni hisoblaymiz, yuqorida ega bo’lgan o’g’irlik va basilar dan foydalangan holda albatta:
</p>
```python
a = model.coef_ * X + model.intercept_
```
Keling hamma vektorlarni bitta grafikda tasvirlaymiz:
```python
axes = plt.gca()
axes.set_ylim([0, 30])
plt.show()
```
![alt text](http://inomjonramatov.uz/images/ml4_1.jpg "Machine Learning part4")

<p style="text-align:justify">
Juda oddiymi? Bir necha qator kod va hammsi bajarilgan! Biz bajargan ish malumotlar bazasini tayyorlash, modelni o’rgatish va natijani vizuallashtirish. Bu albatta scikit-learn kutubxonasining yordami bilan!
<br>Chiziqli regressiya ishini yaxshilash!
<br>
Biz erishgan natija yaxshi, lekin yanada yaxshilashga imkon mavjud. Keling modelimizning aniqlik darajasini tekshirib olamiz, buning uchun bizda score() funksiyasi mavjud: </p>
```python
print(model.score(X, y))
0.84988070842366825
```
<p style="text-align:justify">
Ko’rib turganingizdek biz 85% aniqlik darajasiga erishganmiz , lekin buni yanada yaxshilash yani 90-95% ga olib kelish imkoni mavjud. Qanday qilib qilib? Atributlar sonini oshirish orqali, esingizda bo’lsa atributlar bu obekt belgilari edi, demak obekt belgilari soni oshishi bilan obektni farqlashdagi aniqlik ham oshadi.
<br>
Buning eng sodda yo’li, berilgan atributlar orqali polinomal atributlarni hosil qilish yani, bizda X bor bo’lsa X^2, X^3 kabi atributlarni qo’shish orqali. Keling boshlaymiz:
</p>
```python
X = np.c_[X, X**2]
print(X)
array([[   1,    1],
       [   2,    4],
       [   3,    9],
       [   4,   16],
       [   5,   25],
       [   6,   36],
       [   7,   49],
       [   8,   64],
       [   9,   81],
       [  10,  100]])
```
Yuqorida aytib o'tgan qadamlarni yana qaytadan bajaramiz;
```python
model.fit(X, y)
x = np.arange(1, 11, 0.1)
x = np.c_[x, x**2]
a = np.dot(x, model.coef_.transpose()) + model.intercept_
```
Aktivizatsiya funksiyamizning matematik ko’rinishi quyidagicha: 
![alt text](http://inomjonramatov.uz/images/ml4_2.jpg "Machine Learning part4")
X matritsa ozroq murakkab ko’rinishda, shu sababli endilikda dot funksiyasidan foydalanamiz. Yana biz kichkina x o’zgaruvchini ham yaratdik, aktivizatsiya funksiya grafigi tushunarliroq ko’rinishi uchun. Keling grafikda koramiz:
```python
plt.plot(X[:, 0], y, 'ro', x[:, 0], a)
plt.show()
```
![alt text](http://inomjonramatov.uz/images/ml4_3.jpg "Machine Learning part4")
Ko’rinib turibdiki natija ancha yaxshilandi. Aniqroq raqmlarda ko’ramiz:
```python
model.score(X, y)
0.87215506914951546
```
87 % yuqoriroq aniqlik.

<p style="text-align:justify">
Endi o’ylayotgan bo’lishingiz mumkin, demak polinomal atributlarnin ko’paytirib natijani yanada yaxshilashimiz mumkin ekanda deb! Afsuski unday emas! Keling tekshirib ko’ramiz, 9-darajagacha polinomal atributlarni qo’shamiz:
</p>

```python
X = np.arange(1, 11)
X = np.c_[X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9]
x = np.arange(1, 11, 0.1)
x = np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9]

model.fit(X, y)
a = np.dot(x, model.coef_.transpose()) + model.intercept_

plt.plot(X[:, 0], y, 'ro', x[:, 0], a)
axes = plt.gca()
axes.set_ylim([0, 30])
plt.show()
```

![alt text](http://inomjonramatov.uz/images/ml4_4.jpg "Machine Learning part4")
Aniqlik darajasini raqamlarda ko’ramiz:
```python
model.score(X, y)
0.99992550472904074
```
deyarli 100 % aniqlik. Lekin bu model yaxshiligini anglatmaydi!
<p style="text-align:justify">
OVERFITTING & UNDERFITTING muammolari.<br>
Endi tasavur qiling bizda 15ta misoldan iborat malumotlar bazasi bor edi, lekin men sizga ulardan 10 tasinigina ko'rsatgandim. Quyida qolgan 5 tasi:
</p>
![alt text](http://inomjonramatov.uz/images/ml4_5.jpg "Machine Learning part4")
Demak to’liq malumotlar bazamiz quyidagicha:

```python
X = np.arange(1, 16)
y = np.append(y, [24, 23, 22, 26, 22])

plt.plot(X, y, 'ro')
plt.show()
```
![alt text](http://inomjonramatov.uz/images/ml4_6.jpg "Machine Learning part4")
<p style="text-align:justify">
Keling 100% aniqlikka erishga modelimiz bilan bu malumotlar bazasini tekshirib ko’ramiz:
</p>

```python
plt.plot(X, y, 'ro', x[:, 0], a)
axes = plt.gca()
axes.set_ylim([0, 30])
plt.show()
```

![alt text](http://inomjonramatov.uz/images/ml4_7.jpg "Machine Learning part4")

<p style="text-align:justify">
Ko’ryapsizmi? Bizning model yangi malumotlarni umuman tanimadi! Aniqlik darajasini ko’rib o’tirmasak ham natija yomonligini bilishimiz mumkin.
Mashinali o’rganishda esa model o’rgangan malumotlar bazasi orqali yangi malumotlar bazasini bashorat qilo olishi kerak. Lekin bu model unday qila olmadi. Bu muammo mashinali o’rganishda “overfitting” deb yuritiladi. Model malumotlar bazasini o’rganmadi balki yodlab oldi, va yangi malumotlar bilan ishlay olmadi.
Birinchi yaratgan modelimizni eslaysizmi? Bitta atribut bilan unda aniqlik darajasi juda kichik edi, bu muammo mashinali o’rganishda “underfitting” deb yuritiladi, yani atributlarning yetishmasligi, belgilar kamligi desak ham bo’ladi.
Yuqoridagi muammolardan biz doima yiroqda bo’lishga harakat qilishimiz kerak.  Buning juda ko’p yo’llari o’ylab topilgan, albatta bular haqida ham gaplashamiz.
Bularni hal qilishda birinchi qadam, malumotlar bazasini bo’lish. Yani o’rganiluvchi va test qilinuvchi malumaotlar bazasiga (training and testing datasets). Bu yo’l bilan overfitting problemasini hal qilishimiz mumkin. 
Underfitting ga keladigan bo’lsak aniqlik darajasi ham bu muammo qay darajada ekanligini ko’rsatib beradi va yangi atributlar topishga undaydi.
<br>
Keling malumotlar bazasini bo’lish orqali yaratgan uchala modelimizni tekshirib ko’ramaiz:
</p>

<p style="text-align:justify">
Model 1: 
</p>

```python
X = np.arange(1, 16).reshape(15, 1)
model.fit(X[:10], y[:10])
model.score(X[10:], y[10:])
-12.653810835629017

a = np.dot(X, model.coef_.transpose()) + model.intercept_
plt.plot(X, y, 'ro', X, a)
plt.show()
```

![alt text](http://inomjonramatov.uz/images/ml4_8.jpg "Machine Learning part4")

<p style="text-align:justify">
Model 2: 
</p>

```python
X = np.arange(1, 16).reshape(15, 1)
X = np.c_[X, X**2]
x = np.arange(1, 16, 0.1)
x = np.c_[x, x**2]
model.fit(X[:10], y[:10])
model.score(X[10:], y[10:])
-0.51814820280729812

a = np.dot(x, model.coef_.transpose()) + model.intercept_
plt.plot(X[:, 0], y, 'ro', x[:, 0], a)
axes = plt.gca()
axes.set_ylim([0, 30])
plt.show()
```

![alt text](http://inomjonramatov.uz/images/ml4_9.jpg "Machine Learning part4")

<p style="text-align:justify">
Model 3: 
</p>

```python
X = np.arange(1, 16).reshape(15, 1)
X = np.c_[X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9]
x = np.arange(1, 16, 0.1)
x = np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9]
model.fit(X[:10], y[:10])
model.score(X[10:], y[10:])
-384608869887696.81

a = np.dot(x, model.coef_.transpose()) + model.intercept_
plt.plot(X[:, 0], y, 'ro', x[:, 0], a)
axes = plt.gca()
axes.set_ylim([0, 30])
plt.show()
```

![alt text](http://inomjonramatov.uz/images/ml4_10.jpg "Machine Learning part4")


![alt text](http://inomjonramatov.uz/images/ml3_9.jpg "Machine Learning part3")
<p style="text-align:justify">
Xulosa shundayki, 2- model eng yaxshisi bo’p qolyapti. Eng muhimi bugungi mavzuda mashinali o’rganishda eng ko’p uchraydigan muammolar bilan tanishdik. Malumotlar bazasini 2 ga ajratib o’rganish ham bu muammolarni hal qilishda eng oddiy va foydali usul ekanligini ham bilib oldik. Keyingi mavzularda Mantiqiy regressiya bilan tanishamiz. Ko’rushguncha!
</p>